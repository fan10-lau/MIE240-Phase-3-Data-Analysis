{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme Frequency Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Clean Interview Text â†’ List of Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_interview_text(raw_text):\n",
    "    # Split based on numbered questions (1.) 2.) etc.)\n",
    "    raw_sentences = re.split(r\"\\n?\\d+\\.\\)\", raw_text)\n",
    "    \n",
    "    # Remove extra whitespace and empty strings\n",
    "    cleaned = [s.strip() for s in raw_sentences if s.strip()]\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# for Original Fantuan App \n",
    "# Interview Responses\n",
    "original_raw_text = \"\"\"\n",
    "1.) easy, not hard to do. Annoying thing was the information overload. VIP, ads. That was overwhelming. Other than that, the actual action was fine.  1.5 \n",
    "\n",
    "2.)  Smilar, given I use UBER eats Familiar to me. food types are small, restuarutants icons are too small. I don't like scrolling to much \n",
    "\n",
    "3.) It's too vertical. I barely got to see any food. I want to see the customization options BEFORE I add it to my cart. So I can get it. 3.5\n",
    "\n",
    "4.) Nothing really. Again, show me customization methods  before I add. \n",
    "\n",
    "5.) relatively easy to select a place. The address already being there helped. Way too many pop-up and screens. having to work with a new screen was annoying. Too many screens. 4.5 \n",
    "\n",
    "6.) The sizing of the elemnts are strange. Why are the food type icons too small, while the checkout is good. The sizing isn't very homogenous. Too many pages. \"\n",
    "\"1.) 2. the icon for Korean food was a little ahrd to find. \n",
    "2.) Layout was fine. List format was ok, the fact that they're all scrollable in one long scroll. \n",
    "\n",
    "3.) 6. Ordering the dish was fine, customizing was confusing, I couldn't ell which ones were customizable. It just tells me to add it to cart. NOt crippling, but slightly annoying. \n",
    "4.) consufion about how I can customize stuff was annoying. Having a custmization button would be good, beyond that just being told to add stuff to cart. \n",
    "\n",
    "5.) 1. Since it was already filled out. \n",
    "6.) NOthing comes to mind. \"\n",
    "\"1.)  7. Made me feel stupid. I selected the korean section, it gave me chinese restaurants. \n",
    "2.) It's ok. I don't like how much I have to scroll. \n",
    "\n",
    "3.) 2. Customizng is really nice. It shows me all my options, and all I got to do is click on it. It's all very readable. THIS is all assuming the app actually works. \n",
    "4.) Advantage over the redesign, it can show off the food alot better. I find the full-width banner images for showing off the food is a better fit. I hate this app. \n",
    "\n",
    "5.) 2. It was very easy to select an address, them using geographical data was really helpful as well. ASAP delivery time is default, so I didn't even need to select it. \n",
    "\n",
    "6.) If it works, it's good. But when it doesn't, it's EXTREMLY frustring to use. Working meaning long load times, or the buttons being unresponsive. \"\n",
    "\"1.) 6, it wasn't straightforward and the interface was very complicated. \n",
    "2.) Very crowded and colorful so it was kinda eye-straining.\n",
    "\n",
    "3.) 7, very hard and complicated. Especially the customizing part was very challenging.\n",
    "4.) No, not really.\n",
    "\n",
    "5.) 2, it was easier than the rest of the tasks.\n",
    "6.) I hated the pop-up ads. They were very annoying. Also the overall app was very confusing and crowded. \"\n",
    "\"1.) 5, since it wasn't easy to find a korean restaurant.\n",
    "2.) Although I chose a korean restaurant, the reastaurant that came up were japanese restaurants, whgich was very frustrating.\n",
    "\n",
    "3.) 8, I did not like it at all, it was very overwhelming.\n",
    "4.) When I selected the dish, the app asked me to put my phone number and my email address which was very unnnecessary at that stage.\n",
    "\n",
    "5.) 4, it wasn't that complecated but still it was very crowded.\n",
    "6.) I wish the app wasn't this colorful and messy. \n",
    "\"\"\"\n",
    "\n",
    "# for Team's Redsigned prototype\n",
    "# Interview Responses\n",
    "redesigned_raw_text = \"\"\"\n",
    "1.) 1 (being easy). Options were streamlined. Sizing was more homogneous. \n",
    "2.) Don't like my address at the top. Don't like the deals section being on the main menu, I prefer it being on the restauratn menu, so it feels like I have a choice of what I'm ordering. \n",
    "\n",
    "3.) 2. I like that I can see the options before I added it to my cart. But how would this work on a phone? Woldn't this squish the options? The idea is nice, but I'm worried about the introduction. \n",
    "4.) I would prefer only seeing the dish I am customizing. Why am I seeing the other ones? \n",
    "\n",
    "5.) 7 as in difficult. In the app, it assumed earliet delivery time. What's the pin? I was unsure of what delivery time was. I would prfer seeing the options one at a time, all at once is overwhleming. This feels unnatural. THe optional nature of coupons and merchant message should be designed around. not a fan of the current 9 box layout. \n",
    "\n",
    "6.) The box system feels like it's priming me to pick one box. This sort of translates over to the delivery menu. The devliery options have a chronologically, but how do I figure out that chronology? the boxes lack chronology, I don't know the order in which I need to complete them. LAcking that chronology is an issue, what if one affects the choices of the other. I still have no idea which ones are optinal or not.  I prefer uber eats delivery menu (I've been using it for a long time). \"\n",
    "\"1.) 2. IT felt the same as last time\n",
    "2.) no, not really \n",
    "3.) 3. it was easier to see the customization options. The font was bad. \n",
    "4.) Font was annoying. \n",
    "\n",
    "5.) 5. Some of the information felt like it shold have been autofilled. What the heck was a pin? \n",
    "6.) Overall, the 2 dsigns were really comparable. \"\n",
    "\"1.) 3. The restaurant doesn't immeditaely appear to me as korean. I had to search for the work korean \n",
    "2.) Pretty good. Everything is salient. It's hard to identify what nationarlity the restaurant belongs to . I ogtta read the names. \n",
    "\n",
    "3.) 2. the UI is tiny. There's all this space. Customization menu itself was very compact. Making it larger would be better. I'm unsure if this interface would fit on a phone. The position of the customiation menu wouldn't fit on the food. \n",
    "4.) Menu is nice. Menu was fast. Spacing is good for food. Naming of dishes is good. \n",
    "\n",
    "5.) 2. Why does the coupon button break the pattern established by the previous buttons? Same for the utensils button. The plus button is nice, but it would make snese to tap directly on the box itself. Place button is cute. \n",
    "\n",
    "6.) Progress bar is cool. The minimal design is cool. I'd be totally ok with this interface. Put icons in the delivery section to explain what each fieldd represents. Like a credit-card icon for the payment method section.  the place of the dish customization menu is not good. \"\n",
    "\"1.) 2, it was pretty easy and straighforwared.\n",
    "2.) Everything is pretty basic which makes evrything much more easier so I liked the simplicity.\n",
    "\n",
    "3.) 2, again it was pretty easy.\n",
    "4.) The font of customize section was pretty small so it was kinda hard to read.\n",
    "\n",
    "5.) 1, since the info was already written.\n",
    "6.) Overall I think it looks great, it's very easy to use and I really liked the idea of progress bar instead of scrolling forever :)\"\n",
    "\"1.) 1, very easy.\n",
    "2.) It's intuitive, clear\n",
    "\n",
    "3.) 3, the font wasn't easy to read\n",
    "4.) I think reading the extra toppings part was a little hard but not terrible. \n",
    "\n",
    "5.) 2, becuase I didn't need to write anything\n",
    "6.) I think you dfid a great job, I would use this app instead of the original one for sure.\n",
    "\"\"\"\n",
    "\n",
    "original_responses = re.split(r'\\d\\.\\)', original_raw_text)\n",
    "redesigned_responses = re.split(r'\\d\\.\\)', redesigned_raw_text)\n",
    "\n",
    "# Clean and remove empty strings and extra spaces\n",
    "cleaned_original_responses = [resp.strip().replace('\\n', ' ') for resp in original_responses if resp.strip()]\n",
    "cleaned_redesigned_responses = [resp.strip().replace('\\n', ' ') for resp in redesigned_responses if resp.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Fanyiling/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Fanyiling/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Step 1: Vectorize Responses (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_responses(responses):\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words, max_features=50)\n",
    "    X = vectorizer.fit_transform(responses)\n",
    "    word_freq = X.toarray().sum(axis=0)\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    freq_df = pd.DataFrame({'Word': words, 'Frequency': word_freq})\n",
    "    freq_df = freq_df.sort_values(by='Frequency', ascending=False).reset_index(drop=True)\n",
    "    return freq_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steo 4: Run Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_freq = vectorize_responses(cleaned_original_responses)\n",
    "redesigned_freq = vectorize_responses(cleaned_redesigned_responses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequencies(text_list):\n",
    "    tokens = []\n",
    "    for sentence in text_list:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        words = [w for w in words if w.isalpha() and w not in stop_words]\n",
    "        tokens.extend(words)\n",
    "    return Counter(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Fanyiling/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/Fanyiling/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m original_freq \u001b[38;5;241m=\u001b[39m \u001b[43mget_word_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_responses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m redesigned_freq \u001b[38;5;241m=\u001b[39m get_word_frequencies(redesigned_responses)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(original_freq\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m, in \u001b[0;36mget_word_frequencies\u001b[0;34m(text_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m text_list:\n\u001b[0;32m----> 4\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     words \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m      6\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mextend(words)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Fanyiling/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/Fanyiling/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "original_freq = get_word_frequencies(original_responses)\n",
    "redesigned_freq = get_word_frequencies(redesigned_responses)\n",
    "\n",
    "print(original_freq.most_common(10))\n",
    "print(redesigned_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
